{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook provides an overview of how to:\n",
    "\n",
    "1. Search the OBPG satellite data archive for L1A files\n",
    "2. Output a list of all the granule IDs and download links\n",
    "3. Creates a small database that contains each station information, with the corresponding satellite granule\n",
    "\n",
    "Note: this notebook searches through the [Level 1 & 2 browser](https://oceancolor.gsfc.nasa.gov/cgi/browse.pl?sen=amod), and parses the download links from there. \n",
    "\n",
    "# Requirements\n",
    "\n",
    "This notebook starts from the point of having a field data file which contains the time and location information for each observation *in a particular format*. There's an example file provided in this repository (`exampleStationListFile.csv`), which is printed below. We leave it as an exercise (for now) to get your field data into this format.\n",
    "\n",
    "## Field data station information file format example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Cruise</th>\n",
       "      <th>Station</th>\n",
       "      <th>yyyy-mm-ddThh:mm:ss</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>mins</th>\n",
       "      <th>sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>s980917w_103883</td>\n",
       "      <td>s980917w</td>\n",
       "      <td>103883.0</td>\n",
       "      <td>1998-09-17T13:46:48</td>\n",
       "      <td>-66.307800</td>\n",
       "      <td>43.773600</td>\n",
       "      <td>1998</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>46</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>s980917w_103884</td>\n",
       "      <td>s980917w</td>\n",
       "      <td>103884.0</td>\n",
       "      <td>1998-09-17T13:47:13</td>\n",
       "      <td>-66.310200</td>\n",
       "      <td>43.773200</td>\n",
       "      <td>1998</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>47</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>s980917w_103885</td>\n",
       "      <td>s980917w</td>\n",
       "      <td>103885.0</td>\n",
       "      <td>1998-09-17T13:47:35</td>\n",
       "      <td>-66.312400</td>\n",
       "      <td>43.772900</td>\n",
       "      <td>1998</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>s980917w_103886</td>\n",
       "      <td>s980917w</td>\n",
       "      <td>103886.0</td>\n",
       "      <td>1998-09-17T13:47:56</td>\n",
       "      <td>-66.314700</td>\n",
       "      <td>43.772400</td>\n",
       "      <td>1998</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>s101012e_144471</td>\n",
       "      <td>s101012e</td>\n",
       "      <td>144471.0</td>\n",
       "      <td>2010-10-12T14:59:51</td>\n",
       "      <td>-68.995900</td>\n",
       "      <td>44.052600</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>59</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>s101012e_144472</td>\n",
       "      <td>s101012e</td>\n",
       "      <td>144472.0</td>\n",
       "      <td>2010-10-12T15:01:55</td>\n",
       "      <td>-68.991300</td>\n",
       "      <td>44.050400</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>s101012e_144473</td>\n",
       "      <td>s101012e</td>\n",
       "      <td>144473.0</td>\n",
       "      <td>2010-10-12T15:03:59</td>\n",
       "      <td>-68.985800</td>\n",
       "      <td>44.047700</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>s181205e_2018083.1</td>\n",
       "      <td>s181205e</td>\n",
       "      <td>2018083.1</td>\n",
       "      <td>2018-12-05T04:22:00</td>\n",
       "      <td>-68.444467</td>\n",
       "      <td>43.477100</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>s181206w_2018084.1</td>\n",
       "      <td>s181206w</td>\n",
       "      <td>2018084.1</td>\n",
       "      <td>2018-12-06T10:48:00</td>\n",
       "      <td>-67.150233</td>\n",
       "      <td>43.724533</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>s181206w_2018085.1</td>\n",
       "      <td>s181206w</td>\n",
       "      <td>2018085.1</td>\n",
       "      <td>2018-12-06T12:22:00</td>\n",
       "      <td>-67.471700</td>\n",
       "      <td>43.705533</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID    Cruise    Station  yyyy-mm-ddThh:mm:ss  Longitude  \\\n",
       "0     s980917w_103883  s980917w   103883.0  1998-09-17T13:46:48 -66.307800   \n",
       "1     s980917w_103884  s980917w   103884.0  1998-09-17T13:47:13 -66.310200   \n",
       "2     s980917w_103885  s980917w   103885.0  1998-09-17T13:47:35 -66.312400   \n",
       "3     s980917w_103886  s980917w   103886.0  1998-09-17T13:47:56 -66.314700   \n",
       "4     s101012e_144471  s101012e   144471.0  2010-10-12T14:59:51 -68.995900   \n",
       "5     s101012e_144472  s101012e   144472.0  2010-10-12T15:01:55 -68.991300   \n",
       "6     s101012e_144473  s101012e   144473.0  2010-10-12T15:03:59 -68.985800   \n",
       "7  s181205e_2018083.1  s181205e  2018083.1  2018-12-05T04:22:00 -68.444467   \n",
       "8  s181206w_2018084.1  s181206w  2018084.1  2018-12-06T10:48:00 -67.150233   \n",
       "9  s181206w_2018085.1  s181206w  2018085.1  2018-12-06T12:22:00 -67.471700   \n",
       "\n",
       "    Latitude  year  month  day  hour  mins  sec  \n",
       "0  43.773600  1998      9   17    13    46   48  \n",
       "1  43.773200  1998      9   17    13    47   13  \n",
       "2  43.772900  1998      9   17    13    47   35  \n",
       "3  43.772400  1998      9   17    13    47   56  \n",
       "4  44.052600  2010     10   12    14    59   51  \n",
       "5  44.050400  2010     10   12    15     1   55  \n",
       "6  44.047700  2010     10   12    15     3   59  \n",
       "7  43.477100  2018     12    5     4    22    0  \n",
       "8  43.724533  2018     12    6    10    48    0  \n",
       "9  43.705533  2018     12    6    12    22    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stationListFile = 'example-StationListFile.csv'\n",
    "stationList = pd.read_csv(stationListFile, sep='\\s+')\n",
    "stationList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing other modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import requests\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Search OBPG for satellite granules\n",
    "\n",
    "## Creating a list of search parameters\n",
    "\n",
    "First, let's get a list of cruises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruises = stationList.Cruise.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cruise, we need to get the min and max lats and lons which we can then use for our search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "padding = 0.05 # degrees = ~7km in the Gulf of Maine\n",
    "\n",
    "gnatsSummary = {}\n",
    "for cix,cc in enumerate(cruises):\n",
    "    cruise = stationList.query('Cruise==@cruises.iloc[@cix]')\n",
    "    minlat = np.nanmin(cruise.Latitude) - padding\n",
    "    minlon = np.nanmin(cruise.Longitude) - padding\n",
    "    maxlat = np.nanmax(cruise.Latitude) + padding\n",
    "    maxlon = np.nanmax(cruise.Longitude) + padding\n",
    "\n",
    "    gnatsSummary[cc] = {'minLon' : minlon, 'minLat' : minlat, 'maxLon' : maxlon, 'maxLat' : maxlat}\n",
    "    \n",
    "gnatsSummary = pd.DataFrame(gnatsSummary).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minLon</th>\n",
       "      <th>minLat</th>\n",
       "      <th>maxLon</th>\n",
       "      <th>maxLat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>s980917w</td>\n",
       "      <td>-70.004800</td>\n",
       "      <td>43.565700</td>\n",
       "      <td>-66.257800</td>\n",
       "      <td>43.823600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s980918w</td>\n",
       "      <td>-69.741700</td>\n",
       "      <td>43.576300</td>\n",
       "      <td>-66.248400</td>\n",
       "      <td>43.827400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s980929w</td>\n",
       "      <td>-69.408000</td>\n",
       "      <td>43.603200</td>\n",
       "      <td>-66.216522</td>\n",
       "      <td>43.825855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s981004w</td>\n",
       "      <td>-70.034804</td>\n",
       "      <td>43.560517</td>\n",
       "      <td>-66.256700</td>\n",
       "      <td>43.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s981005w</td>\n",
       "      <td>-69.707900</td>\n",
       "      <td>43.576500</td>\n",
       "      <td>-66.142300</td>\n",
       "      <td>43.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s180917w</td>\n",
       "      <td>-69.853000</td>\n",
       "      <td>43.516100</td>\n",
       "      <td>-66.193700</td>\n",
       "      <td>43.818500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s181001w</td>\n",
       "      <td>-69.867900</td>\n",
       "      <td>43.516200</td>\n",
       "      <td>-66.180000</td>\n",
       "      <td>43.821100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s180823w</td>\n",
       "      <td>-69.826900</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>-66.270100</td>\n",
       "      <td>43.816400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s181205e</td>\n",
       "      <td>-70.473200</td>\n",
       "      <td>43.048100</td>\n",
       "      <td>-67.107000</td>\n",
       "      <td>43.773500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s181206w</td>\n",
       "      <td>-69.817100</td>\n",
       "      <td>43.514900</td>\n",
       "      <td>-67.099200</td>\n",
       "      <td>43.776900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             minLon     minLat     maxLon     maxLat\n",
       "s980917w -70.004800  43.565700 -66.257800  43.823600\n",
       "s980918w -69.741700  43.576300 -66.248400  43.827400\n",
       "s980929w -69.408000  43.603200 -66.216522  43.825855\n",
       "s981004w -70.034804  43.560517 -66.256700  43.828600\n",
       "s981005w -69.707900  43.576500 -66.142300  43.828600\n",
       "...             ...        ...        ...        ...\n",
       "s180917w -69.853000  43.516100 -66.193700  43.818500\n",
       "s181001w -69.867900  43.516200 -66.180000  43.821100\n",
       "s180823w -69.826900   0.016700 -66.270100  43.816400\n",
       "s181205e -70.473200  43.048100 -67.107000  43.773500\n",
       "s181206w -69.817100  43.514900 -67.099200  43.776900\n",
       "\n",
       "[204 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnatsSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformatting the cruise date into unix epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtstring = gnatsSummary.index\n",
    "pre2000 = dtstring.str.contains('s9')\n",
    "year = np.ones(pre2000.shape).astype(int)\n",
    "year[pre2000] = [int(dts[1:3])+1900 for dts in dtstring[pre2000]]\n",
    "year[~pre2000] = [int(dts[1:3])+2000 for dts in dtstring[~pre2000]]\n",
    "month = [int(dts[3:5]) for dts in dtstring]\n",
    "day = [int(dts[5:7]) for dts in dtstring]\n",
    "\n",
    "#dividing by 1 day to end up with float rather than a datetime object\n",
    "unixTime = [int((dt.datetime(yy,mm,dd) - dt.datetime(1970,1,1)) / dt.timedelta(days=1)) \\\n",
    "            for (yy,mm,dd) in zip(year,month,day)]\n",
    "gnatsSummary['unixTime'] = unixTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for satellite granules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function to craft the urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crafturl(cruise,sensor):\n",
    "    # sensor must be: amod, tmod, vrsn, swml\n",
    "    # cruise must be one row of a dataframe, which contains:\n",
    "    # unixTime = start time in unix time\n",
    "    # maxLon, maxLat, minLon, minLat = bounding coordinates for the\n",
    "    #                 region of interst\n",
    "\n",
    "    # checking sensor strings\n",
    "    if sensor not in ['amod','tmod','vrsn','swml','swg']:\n",
    "        raise Exception('Unrecognized sensor string must be one of:' + \\\n",
    "                        ' amod, tmod, vrsn, swml, swg' )\n",
    "    \n",
    "    # crafting search url\n",
    "    browserurl = 'https://oceancolor.gsfc.nasa.gov/cgi/browse.pl'\n",
    "    url = browserurl + \\\n",
    "            '?sub=level1or2list&sen=' + sensor + \\\n",
    "            '&per=DAY&day=' + cruise.unixTime + \\\n",
    "            '&w=' + cruise.minLon+ '&s=' + cruise.minLat + \\\n",
    "            '&e=' + cruise.maxLon + '&n=' + cruise.maxLat\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to parse the L1/2 brower results webpage html. There are two paths here depending on the results of the search. If multiple files match the search, we have to parse the thumbnails page, and then for each thumbnail, go to the individual swath page, parse that and get our download link. However, if only one file matches the search, we go straight to the individual swath page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through all cruises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sensors = ['amod','tmod','vrsn','swml']\n",
    "granules = {}\n",
    "for cruise in gnatsSummary.astype(str).itertuples():\n",
    "    granids = []\n",
    "    granlinks = []\n",
    "    for sensor in sensors:\n",
    "        url = crafturl(cruise,sensor)\n",
    "\n",
    "        # do the L1/L2 browser search\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except SSLError:\n",
    "            print('SSLError in ('+cruise+', '+sensor+') continuing with next search...')\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        #status code = 200 means search was successful and\n",
    "        #there is content on the found page\n",
    "        if req.status_code == 200: \n",
    "            # parsing the url html into an element tree\n",
    "            htmltree = html.fromstring(req.content)\n",
    "\n",
    "            # checking the title of the page\n",
    "            title = htmltree.xpath('//title')[0].text\n",
    "\n",
    "            #search html tags for <a href...>\n",
    "            #double slash means return all tags\n",
    "            refs = htmltree.xpath('//a')\n",
    "\n",
    "            if \"Thumbnail\" in title:\n",
    "                for ref in refs:\n",
    "                    link = ref.get('href','')\n",
    "                    regex = re.compile('[A-Z][0-9]{13}\\.L2')\n",
    "                    if regex.search(link):\n",
    "                        #there's 2 hyperlinks for each file, one on the file name, and one\n",
    "                        #on the image thumbnail - we just need one\n",
    "                        brws_regex = re.compile('_BRS_BRS')\n",
    "                        if not brws_regex.search(link):\n",
    "                            fullurl = 'https://oceancolor.gsfc.nasa.gov'+link\n",
    "\n",
    "                            #parsing the webpage for each satellite file to get L1A download link\n",
    "                            #(using the same procedure as above)\n",
    "                            try:\n",
    "                                req2 = requests.get(fullurl)\n",
    "                            except SSLError:\n",
    "                                print('SSLError in ('+cruise+', '+sensor+') thumbnail continuing with next search...')\n",
    "                                continue\n",
    "                                \n",
    "                            htmltree2 = html.fromstring(req2.content)\n",
    "                            refs2 = htmltree2.xpath('//a')\n",
    "                            for ref in refs2:\n",
    "                                link2 = ref.get('href','')\n",
    "                                regex2 = re.compile('[A-Z][0-9]{13}\\.L1A')\n",
    "                                if regex2.search(link2):\n",
    "                                    granlinks += [link2]\n",
    "                                    granids += [re.search('[A-Z][0-9]{13}',link2).group()]\n",
    "            else:\n",
    "                #parsing the webpage for each satellite file to get L1A download link\n",
    "                for ref in refs:\n",
    "                    link2 = ref.get('href','')\n",
    "                    regex2 = re.compile('[A-Z][0-9]{13}\\.L1A')\n",
    "                    if regex2.search(link2):\n",
    "                        granlinks += [link2]\n",
    "                        granids += [re.search('[A-Z][0-9]{13}',link2).group()]\n",
    "\n",
    "    granules[cruise.Index] = {'granid' : granids, 'granlinks' : granlinks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformatting as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "granule_dataframes = []\n",
    "for cruise,graninfo in granules.items():\n",
    "    df = pd.DataFrame(graninfo)\n",
    "    df['cruise'] = [cruise]*len(df)\n",
    "    granule_dataframes += [df]\n",
    "    \n",
    "granulesDF = pd.concat(granule_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>granid</th>\n",
       "      <th>granlinks</th>\n",
       "      <th>cruise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>S1998260183114</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s980917w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>S1998260165214</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s980917w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>S1998261173708</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s980918w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>S1998261155809</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s980918w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>S1998272173530</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s980929w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A2018340183500</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s181206w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A2018340170000</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s181206w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>T2018340151500</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s181206w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>V2018340174200</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s181206w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>V2018340160000</td>\n",
       "      <td>https://oceandata.sci.gsfc.nasa.gov/ob/getfile...</td>\n",
       "      <td>s181206w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1075 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            granid                                          granlinks  \\\n",
       "0   S1998260183114  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "1   S1998260165214  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "0   S1998261173708  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "1   S1998261155809  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "0   S1998272173530  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "..             ...                                                ...   \n",
       "0   A2018340183500  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "1   A2018340170000  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "2   T2018340151500  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "3   V2018340174200  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "4   V2018340160000  https://oceandata.sci.gsfc.nasa.gov/ob/getfile...   \n",
       "\n",
       "      cruise  \n",
       "0   s980917w  \n",
       "1   s980917w  \n",
       "0   s980918w  \n",
       "1   s980918w  \n",
       "0   s980929w  \n",
       "..       ...  \n",
       "0   s181206w  \n",
       "1   s181206w  \n",
       "2   s181206w  \n",
       "3   s181206w  \n",
       "4   s181206w  \n",
       "\n",
       "[1075 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "granulesDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Output list of granule IDs and download links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reordering the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_overpasses = granulesDF[['cruise','granid','granlinks']]\n",
    "satellite_overpasses = satellite_overpasses.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_overpasses.to_csv('satellite-overpasses.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create a small database that contains each station information, with the corresponding satellite granule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique cruise list to loop through:\n",
    "cruiseList = satellite_overpasses.cruise.drop_duplicates()\n",
    "\n",
    "matchups = []\n",
    "for cruise in cruiseList:\n",
    "    #overpasses for this cruise\n",
    "    overpasses = satellite_overpasses[satellite_overpasses.cruise.str.contains(cruise)]\n",
    "    \n",
    "    #stations for this cruise, with dates & times reformatted into Python datetime objects\n",
    "    stations = stationList[stationList.Cruise.str.contains(cruise)]\n",
    "    dt_stations = [dt.datetime(yy,mm,dd,hh,mn,0) for (yy,mm,dd,hh,mn)  \n",
    "                   in zip(stations.year,stations.month,stations.day,stations.hour,stations.mins)]\n",
    "    stations.loc[:,'datetimes'] = dt_stations\n",
    "    \n",
    "    #looping through each satellite\n",
    "    cruisematchups = []\n",
    "    for op in overpasses.itertuples():\n",
    "        \n",
    "        #getting granule info\n",
    "        granule = op.granid\n",
    "        mm = int(granule[10:12])\n",
    "        hh = int(granule[8:10])\n",
    "        yyyy = int(granule[1:5])\n",
    "        doy = int(granule[5:8])\n",
    "\n",
    "        #formatting granule date and time into Python datetime object\n",
    "        ymd = dtc.DOYtoYYYYMMDD(doy,yyyy).astype(int)\n",
    "        dt_granule = dt.datetime(ymd[0,0],ymd[0,1],ymd[0,2],hh,mm,0)\n",
    "\n",
    "        #comparing times\n",
    "        timediff = [dt_granule - dts for dts in stations['datetimes']]\n",
    "        timediff_hrs = [td.days*24 + td.seconds/3600 for td in timediff]\n",
    "        time_idx = [np.absolute(td) <= 3. for td in timediff_hrs] #using a 3 hr window\n",
    "\n",
    "        #indexing stations\n",
    "        matched = stations.loc[time_idx]\n",
    "\n",
    "        #adding in granule info\n",
    "        if len(matched) > 0:\n",
    "            matched.loc[:,'granid'] = granule\n",
    "            cruisematchups += [matched]   \n",
    "    \n",
    "    # converting matchups into a dataframe and adding to a master matchup list\n",
    "    if len(cruisematchups) > 0:\n",
    "        matchups += [pd.concat(cruisematchups)]\n",
    "\n",
    "matchupList = pd.concat(matchups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchupList.to_csv('matchupStationList.csv',sep='\\t',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
